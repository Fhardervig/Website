<!DOCTYPE html>

<head>
    <title>Dirac Notation: The Essentials</title>
    <link rel="icon" type="image/x-icon" href="./favicon-32x32.png">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
    <script src="https://unpkg.com/@popperjs/core@2"></script>
    <script src="https://unpkg.com/tippy.js@6"></script>
    <link rel="stylesheet" href="https://unpkg.com/tippy.js@6/themes/material.css" />

    <!-- Run MathJax -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [
                    ['$', '$'],
                    ['\\(', '\\)']
                ],
                tags: 'ams'
            },
            startup: {
                pageReady() {
                    return MathJax.startup.defaultPageReady().then(function() {
                        $.getScript("./resources/diracNotation.js");
                        $.getScript("./resources/main.js");
                    });
                }
            }
        }
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel='stylesheet' href="./resources/theme.css">
    <link rel='stylesheet' href='./resources/style.css'>
    <link rel='stylesheet' href='./resources/dragula_style.css'>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/tiny-slider/2.9.4/tiny-slider.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/tiny-slider/2.9.2/min/tiny-slider.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/dragula/3.6.6/dragula.min.js" integrity="sha512-MrA7WH8h42LMq8GWxQGmWjrtalBjrfIzCQ+i2EZA26cZ7OBiBd/Uct5S3NP9IBqKx5b+MMNH1PhzTsk6J9nPQQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
</head>



<body>
    <br>
    <time pubdate datetime="2021-12-06" title="December 6th, 2021"><span>DEC.06.2021</span></time>

    <h1>
        Dirac Notation: The essentials
    </h1>

    <address class="author"><span>By Frederik Hardervig</span></address>
    <article>
        <section>
            <h2>Complex Numbers</h2>
            <p>I will assume you're already somewhat comfortable with complex numbers, where we introduce the imaginary unit $i=\sqrt{-1}$ to construct complex numbers of the form $z=a+bi$. In quantum mechanics, we often encounter <em>complex conjugates</em>                and the <em>euclidean norm</em>, which we will simply refer to as the <em>norm</em>.</p>
            <p>The complex conjugate of a complex number $z=a+bi$ is denoted with an asterisk, *, and given as $z^\ast=a-bi$. It's effect is to flip the sign of the imaginary part of any complex number, which also means that it is its <span class="itTooltip"
                    data-content="In math terms, we'd say it's an <em>involution</em>.">own inverse</span>: $$\left(z^\ast\right)^\ast=\left((a+bi)^\ast\right)^\ast=\left(a-bi\right)^\ast=a+bi=z$$ It is distributive with matrix-like objects, e.g. the vector
                $\vec{v}=\begin{pmatrix}a+bi\\c+di\end{pmatrix}$ becomes $\vec{v}^\ast=\begin{pmatrix}a-bi\\c-di\end{pmatrix}$. The complex conjugate has no effect on real numbers as there is no imaginary part to change the sign of, e.g. $3^\ast=3$, while
                it's like multiplying by $-1$ for purely imaginary numbers e.g. $3i^\ast=-3i$.</p>
            <p>The norm of a complex number, $z=a+bi$ is given as $$|z|=\sqrt{zz^\ast}=\sqrt{(a+bi)(a-bi)}=\sqrt{a^2+b^2}$$ and carries with it a notion of magnitude, as it returns a real number that's equal to the length of the vector $(a,b)$ in <span class="itTooltip"
                    data-content="I.e. a 2 dimensional coordinate system, where each coordinate is a real number.">$\mathbb{R}^2$</span>. Now, we're ready to introduce Dirac notation.</p>
        </section>
        <section>
            <h2>The Ket</h2>
            <p>
                At the heart of Dirac notation lies the <strong> ket</strong>, $\ket{\phantom{\psi}}$, which is used to denote the <span class="itTooltip" data-content="In general you can work with quantum mechanics from the perspective of wave mechanics or matrix mechanics, which are equivalent. <br>We will focus on matrix mechanics.">vector form</span>                of a <strong>quantum state</strong>. E.g. we could write a quantum state <span class="itTooltip" data-content="$\psi$ and $\varphi$ are commonly used to denote arbitrary quantum states, but we could just as well call them $Q$ and $\epsilon$."> $\psi$</span>                as $\ket{\psi}$. Mathematically, the ket is a vector in a <span class="itTooltip" data-content="If you're curious, <a href='https://www.w3schools.com'>here</a> is another post explaining what a Hilbert space is.">Hilbert space</span>,
                which, for now, you should just know is a <strong>vector of complex numbers</strong>, $$\ket{\psi}=\begin{pmatrix}a\vec{e_1}\\b\vec{e_2}\\\vdots\\n\vec{e_n}\end{pmatrix}$$ where $a,b,...,n\in\mathbb{C}$ and $\{\vec{e_1},\vec{e_2},...,\vec{e_n}\}$
                is our basis.</p>
            <p>
                For now, we will constrain out kets to represent finite vectors, but once we move on to wave mechanics, $\ket{\psi}$ might instead be some vector-valued function dependent on position and time $\ket{\psi(x,y,z,t)}$. Throughout this article, we will use
                the spin state of an electron as our example, but note that <strong>kets can have infinitely many dimensions</strong>.</p>
            <p>
                For a simple system, such as the spin state of an electron, once we measure, it can either be up or down, which we'll denote as $\ket{\uparrow}$ and $\ket{\downarrow}$ respectively, and which we'll use as our basis. Since there are two, we'll need a 2
                dimensional vector to explain this, and we let $$\ket{\uparrow}=\begin{pmatrix}1\\0\end{pmatrix} \;\text{ and }\;Â \ket{\downarrow}=\begin{pmatrix}0\\1\end{pmatrix}$$ to form an orthonormal basis.
                <strong>Remember the numerical vectors that these kets represent!</strong> They will make the remaining text much more understandable!
            </p>
            <section class="subsection">
                <h3>Superposition</h3>
                <p>
                    Note that I mentioned that the spin of the electron can be either up or down <em>upon measuring</em>. This is a crucial distinction since the quantum state before measurement can be a <span class="itTooltip" data-content="A linear combination is given as $C = a\mathbf{x}+b\mathbf{y}...+n\mathbf{z}$ where $a,b,c$ are scalars, and $x,y,z$ are the objects we're combining.">linear combination</span>                    of up and down.
                </p>
                <p>To explain superposition in the context of qubits, I will present the analogy of flipping a coin. Upon landing, it can only be either heads or tails, but while spinning in the air, one could argue that its state is better described as
                    "50% heads and 50% tails".
                    <span class="itTooltip" data-content="<a href='https://youtu.be/kv-YXKRUheQ' target='_blank'>This</a> computerphile video does an excellent job of clarifying this, and gives a good introduction to the idea of superposition."><strong>The coin is not both heads and tails at the same time</strong></span>                    as some pop-sci articles might lead you to believe. If you think about the probability of each outcome, then instead of the coin "being both at the same time", you simply add them together to get the probability distribution of the
                    state. This is what is meant by superposition, and to complete the analogy you can think of heads as $\ket{\uparrow}$ and tails as $\ket{\downarrow}$.</p>
                <img src="./Linear Combination2.png" style="object-fit:contain; max-width:100%;
max-height:100%;">
                <figcaption>By creating a linear combination of the outcome probabilities, we can describe the superposition between heads and tails.</figcaption>

                <p> In quantum mechanics, we will not exactly be weighing the outcomes by their probabilities directly, but rather by their <strong>probability amplitudes</strong>, e.g.$$\ket{\psi}=\alpha\ket{\uparrow}+\beta\ket{\downarrow}=\begin{pmatrix}\alpha\\\beta\end{pmatrix}$$
                    where $\alpha$ and $\beta$ are the probability amplitudes which are <strong>complex numbers</strong>. The actual probability is obtained by taking the <span class="itTooltip" data-content="Remember that the norm of a complex number, $z=a+bi$, is given as $$\begin{align*}|z|=\sqrt{zz^\ast}&=\sqrt{(a+bi)(a-bi)}\\&=\sqrt{a^2+b^2}\end{align*}$$">norm squared</span>                    of our amplitude. If we let our probability amplitude of spin up be the complex number $\alpha=a+bi$, then the probability of spin up, $P_\uparrow$, is given as $$\begin{equation}P_\uparrow=|\alpha|^2=\sqrt{a^2+b^2}^2=a^2+b^2 \label{eq:Pup}\end{equation}$$
                    while the probability of spin down, $P_\downarrow$, is given as $$P_\downarrow=|\beta|^2=\sqrt{c^2+d^2}^2=c^2+d^2$$ where we let $\beta=c+di$.</p>

            </section>
            <p>
                To re-cap, we can describe quantum states as <span id="ilqKet1">____</span> vectors, with entries corresponding to the <span id="ilqKet2">____</span> of measuring an outcome as the corresponding basis vector.
            </p>
            <p>We will now do a small detour from Dirac notation, to introduce some matrix operations.</p>
        </section>
        <section>
            <h2>Transpose</h2>
            <p>To <em>transpose</em> is to switch the rows and columns in matrix-like objects while making no changes to the entries themselves. E.g. $$\begin{bmatrix}a&b\\c&d\\e&f\end{bmatrix}^T=\begin{bmatrix}a&c&e\\b&d&f\end{bmatrix} \;\text{ and }\;
                \begin{pmatrix}a\\b\\c\end{pmatrix}^T=\begin{pmatrix}a&b&c\end{pmatrix}$$ Like the complex conjugate, transposing is an involution, i.e. transposing the same matrix twice leaves us back with the initial matrix: $$\left(A^T\right)^T=\left(\begin{bmatrix}a&b\\c&d\end{bmatrix}^T\right)^T=\begin{bmatrix}a&c\\b&d\end{bmatrix}^T=\begin{bmatrix}a&b\\c&d\end{bmatrix}=A$$

            </p>
        </section>
        <section>
            <h2>Adjoint</h2>
            <p>The <span class="itTooltip" data-content="Sometimes called the conjugate transpose, Hermitian transpose or Hermitian conjugate"><em>adjoint</em></span> of a matrix-like object is the <strong>same as taking the complex conjugate of the
                transposed</strong> matrix. In quantum mechanics it is denoted with a dagger, $A^\dagger$. Letting $$\begin{align*} A&=\begin{bmatrix}a+bi&c+di\\e+fi&g+hi\end{bmatrix}\\ &\text{then}\\ A^\dagger&={\left(A^\ast\right)^T}\\&=\begin{bmatrix}a-bi&c-di\\e-fi&g-hi\end{bmatrix}^T\\&=\begin{bmatrix}a-bi&e-fi\\c-di&g-hi\end{bmatrix}
                \end{align*}$$ Just like both transposing and complex conjugates are involutions, it also holds that $\left(A^\dagger\right)^\dagger=A$. Feel free to confirm this by yourself using the example above.</p>
        </section>
        <section>
            <h2>Bra</h2>
            Returning to our electron spin quantum state, we can take the adjoint of $\ket{\psi}$ in order to obtain: $$\ket{\psi}^\dagger=\begin{pmatrix}\alpha\\\beta\end{pmatrix}^\dagger=\begin{pmatrix}\alpha^\ast&\beta^\ast\end{pmatrix}=\bra{\psi}$$ What's this,
            our ket flipped? That's right, we're introducing the one and only <strong>bra</strong>, $\bra{\phantom{\psi}}$. The adjoint even let's us convert back and forth between kets and bras. $$\bra{\psi}^\dagger=\begin{pmatrix}\alpha^\ast&\beta^\ast\end{pmatrix}^\dagger=\begin{pmatrix}\alpha\\\beta\end{pmatrix}=\ket{\psi}$$
        </section>
        <section>
            <h2>
                Inner Product
            </h2>
            <p>Finally we know enough to begin seeing why Dirac notation might be useful. The vertical bars almost seems to fit together, and writing $\braket{\psi|\psi}$ we've made our first <strong>bra-ket pair</strong>! We say that this will give us the
                inner product of $\ket{\psi}$ with respect to itself. The <em>inner product</em> is an extension of the dot product, but will in this reading behave the exact same. As such, it takes in two vectors and <strong>returns a scalar</strong>.
                Writing out the vectors and performing matrix multiplication, we obtain: $$\begin{equation}\braket{\psi|\psi}=\begin{pmatrix}\alpha^\ast&\beta^\ast\end{pmatrix}\begin{pmatrix}\alpha\\\beta\end{pmatrix}=\alpha^\ast\alpha+\beta^\ast\beta=\class{tooltip}{\cssId{sumNormSqrd}{|\alpha|}}^2+|\beta|^2\qquad
                \label{eq:selfprj}\end{equation}$$ This looks familiar, namely in eq. $\ref{eq:Pup}$, we saw that $P_\uparrow=|\alpha|^2$, and by extension that $P_\downarrow=|\beta|^2$. As such, by taking the inner product of a state with respect to
                itself we've extracted the sum of probabilities of all outcomes. </p>
            <p>For this to make sense physically we need to have a properly normalized state such that, $\braket{\psi|\psi}=1$, much like we need the integral of probability density functions to <span class="itTooltip" data-content="In other words, upon measuring the particle must always be in <em>some</em> state. It's not allowed to simply cease to exist.">equal 1</span>.
                Note that the inner product of a quantum state with itself, $\braket{\psi|\psi}$, is a special case of the inner product where the resulting probability amplitude and associated probability is the same.
            </p>
            <p>While this fact is useful for normalizing our states, the inner product also serves as an exceedingly useful tool since we can use it to extract probability amplitudes of <strong>specific outcomes</strong>. E.g. $$\begin{equation}\braket{\uparrow|\psi}=\begin{pmatrix}1&0\end{pmatrix}\begin{pmatrix}\alpha\\\beta\end{pmatrix}=1\alpha+0\beta=\alpha\label{eq:ampup}\end{equation}$$
                where we recall that $\alpha$ is the probability amplitude of $\ket{\psi}$ for the outcome $\ket{\uparrow}$. From eq. \ref{eq:selfprj} and eq. \ref{eq:ampup}, we see that $$\braket{\psi|\psi}=|\alpha|^2+|\beta|^2=\left|\braket{\uparrow|\psi}\right|^2+\left|\braket{\downarrow|\psi}\right|^2$$
                which hints at the fact that we might be able to define an alternative construction of our quantum state by using $\braket{e_n|\psi}$ as the amplitude for basis vector $\vec{e_n}$. For our two state system, instead of $$\ket{\psi}=\alpha\ket{\uparrow}+\beta\ket{\downarrow}=\begin{pmatrix}\alpha\\\beta\end{pmatrix}$$
                we could say $$\ket{\psi}=\class{tooltip}{\cssId{numvec}{\braket{\uparrow|\psi}\ket{\uparrow}}}+\braket{\downarrow|\psi}\ket{\downarrow}=\begin{pmatrix}\alpha\\\beta\end{pmatrix}$$ For higher dimensional states with orthonormal basis $\{\vec{e_1},\vec{e_2},...,\vec{e_n}\}$,
                with probability amplitudes $a_1, a_2, ..., a_n$, $$\begin{equation}\ket{\psi}=\sum_{i=1}^n a_i\ket{e_i}=\sum_{i=1}^n\braket{e_i|\psi}\ket{e_i}\label{eq:sumstate}\end{equation}$$
            </p>
            <div class="example tight">
                <button class="collapsible">Aside: Inner products as projections</button>
                <div class="content">
                    <p>
                        The following purely for some geometric intuition and is not strictly necessary. <br>Recall that inner products are a generalization of dot products, and that dot products can be used to project one vector, $\vec{v}$, onto another,
                        $\vec{b}$, by using $\vec{v}_{proj\rightarrow b}=(\hat{b}\cdot\vec{v})\hat{b}$ where $\hat{b}$ is the unit vector of $\vec{b}$. As mentioned above, for our quantum states to make physical sense, any ket we make must be normalized,
                        and thus the unit vector of any state is the state itself $\hat{\ket{\psi}}=\ket{\psi}$. Re-writing the projection above in Dirac notation, we thus obtain $\ket{\psi_{proj\rightarrow \phi}}=$<span id="ilqInPP1">____</span>, which
                        looks exactly like the summation in eq. \ref{eq:sumstate}.
                    </p>
                </div>

            </div>
            <p>The inner product carries with it a notion of orthogonality. E.g. we mentioned that our basis $\{\ket{\uparrow},\ket{\downarrow}\}$ is orthonormal. This can be easily confirmed using inner products. $$\begin{align*} \braket{\uparrow|\uparrow}&=\begin{pmatrix}1&0\end{pmatrix}\begin{pmatrix}1\\0\end{pmatrix}=1\cdot1+0\cdot0=1\\
                \braket{\uparrow|\downarrow}&=\begin{pmatrix}1&0\end{pmatrix}\begin{pmatrix}0\\1\end{pmatrix}=0\cdot1+1\cdot0=0\\ \braket{\downarrow|\uparrow}&=\begin{pmatrix}0&1\end{pmatrix}\begin{pmatrix}1\\0\end{pmatrix}=1\cdot0+0\cdot1=0\\ \braket{\downarrow|\downarrow}&=\begin{pmatrix}0&1\end{pmatrix}\begin{pmatrix}0\\1\end{pmatrix}=0\cdot0+1\cdot1=1
                \end{align*}$$ Using our intuition from dot products, we see that $\ket{\uparrow}$ andÂ $\ket{\downarrow}$ are orthogonal since $\braket{\uparrow|\downarrow}=\braket{\downarrow|\uparrow}=0$, and normalized since $\braket{\uparrow|\uparrow}=\braket{\downarrow|\downarrow}=1$.
                <div class="focus">
                    It is very common to work with orthonormal bases like these, which means that $\braket{e_i|e_j}=\delta_{ij}$, where $\delta_{ij}$ is the Kronecker delta defined as $\delta_{ij}=\left\lbrace\begin{matrix} 0\quad \text{if } i\neq j\\1\quad \text{if } i=
                    j \end{matrix}\right.$
                </div>
            </p>
            <p>Re-doing our calculation for $\braket{\psi|\psi}$ where instead of using $\ket{\psi}=\begin{pmatrix}\alpha\\\beta\end{pmatrix}$ we will use $\ket{\psi}=\alpha\ket{\uparrow}+\beta\ket{\downarrow}$, and the fact that we have a orthonormal basis:
                $$\begin{align*} \braket{\psi|\psi}&=\left(\alpha^\ast\bra{\uparrow}+\beta^\ast\bra{\downarrow}\right)\left(\alpha\ket{\uparrow}+\beta\ket{\downarrow}\right) \\ &=\alpha^\ast\alpha\underbrace{\braket{\uparrow|\uparrow}}_{\delta_{\uparrow\uparrow}=\,1}+\alpha^\ast\beta\underbrace{\braket{\uparrow|\downarrow}}_{\delta_{\uparrow\downarrow}=\,0}+\beta^\ast\alpha\underbrace{\braket{\downarrow|\uparrow}}_{\delta_{\downarrow\uparrow}=\,0}+\beta^\ast\beta\underbrace{\braket{\downarrow|\downarrow}}_{\delta_{\downarrow\downarrow}=\,1}\\
                &=|\alpha|^2+|\beta|^2 \end{align*}$$ where we've remember that the norm is given as $|\alpha|=\sqrt{\alpha^\ast \alpha}$. Once we move onto an $x,y,z$ basis, the usefulness of this fact will become even clearer, but already now, the intuitiveness
                of Dirac notation should begin to shine through.</p>

            <p>To re-cap: Inner products extract probability amplitudes. The bra-ket pair takes two <span id="ilqInP1">____</span> and outputs a <span id="ilqInP2">____</span>. The ket is our quantum state which we project onto the bra, in order to obtain
                the probability amplitude of the ket being in the state of the bra. This is an incredibly powerful tool and as we've seen can also be used to define our quantum states. Lastly, by working with orthonormal bases we can quickly remove terms
                and simplify our equations using the <span id="ilqInP3">____</span> delta.
            </p>
        </section>
        <section>
            <h2>
                Operators
            </h2>
            <p>Operators are matrices, and <strong>act as transformations</strong> on our quantum states, just like the transformation matrices from linear algebra. To apply an operator to a ket, you left multiply, $X\ket{\psi}$, while you right multiply
                to apply them to bras, $\bra{\psi}Y$. Being matrices, they are commutative and associative $$\begin{align*} X+Y&=Y+X\\ X+(Y+Z)&=(X+Y)+Z \end{align*}$$ and all the operators we will be encountering are linear, they satisfy $$X(c_\psi\ket{\psi}+c_\phi\ket{\phi})=c_\psi
                X\ket{\psi}+c_\phi X\ket{\phi}$$</p>
            <p>Importantly, <strong>in general, operators do not <span class="itTooltip" data-content="We'll get back to where this word comes from at the end of this article"><em>commute</em></span></strong>, i.e. $XY\neq YX$.<br>Operators are said to be
                <em>Hermitian</em> if $X=X\class{tooltip}{\cssId{dagger}{^\dagger}}$. Operators are said to be <em>unitary</em> if $U^\dagger U=UU^\dagger =\mathbb{I}$, where $\mathbb{I}$ is the identity matrix. Notice that the adjoint is distributed
                the following way: $$(XY)^\dagger=Y^\dagger X^\dagger$$
            </p>
        </section>
        <!-- 







                
            -->
        <div class="draggable">
            <h3>Drag these quantities to the cells below</h3>
            <div class="top" data-answer="default">
                <div class="draggable-element" data-answer="dependent" data-correct-text="True! There's a $t$ in $$\left\langle \hat{x}\right\rangle_\alpha(t)=\sqrt{\frac{2\hbar}{m\omega}}\left|\alpha_0\right|\cos{\left(\omega t-\phi\right)}$$" data-wrong-text="Not quite. Remember that $$\left\langle \hat{x}\right\rangle_\alpha(t)=\sqrt{\frac{2\hbar}{m\omega}}\left|\alpha_0\right|\cos{\left(\omega t-\phi\right)}$$">The expectation value of position, $\left\langle \hat{x}\right\rangle_\alpha(t)$ </div>
                <div class="draggable-element" data-answer="independent" data-correct-text="True! There's no $t$ in $$\Delta\hat{x}_\alpha=\sqrt{\frac{\hbar}{2m\omega}}$$" data-wrong-text="Not quite. Remember that $$\Delta\hat{x}_\alpha=\sqrt{\frac{\hbar}{2m\omega}}$$">The uncertainty of the position operator, $\Delta\hat{x}_\alpha$ </div>



                <div class="draggable-element" data-answer="dependent" data-correct-text="True! There's a $t$ in $$\left\langle \hat{p}\right\rangle_\alpha(t)=-\sqrt{2m\hbar\omega}\left|\alpha_0\right|\sin{\left(\omega t-\phi\right)}$$" data-wrong-text="Not quite. Remember that $$\left\langle \hat{p}\right\rangle_\alpha(t)=-\sqrt{2m\hbar\omega}\left|\alpha_0\right|\sin{\left(\omega t-\phi\right)}$$">The expectation value of momentum, $\left\langle \hat{p}\right\rangle_\alpha(t)$ </div>
                <div class="draggable-element" data-answer="independent" data-correct-text="True! There's no $t$ in $$\Delta\hat{p}_\alpha=\sqrt{\frac{m\hbar\omega}{2}}$$" data-wrong-text="Not quite. Remember that $$\Delta\hat{p}_\alpha=\sqrt{\frac{m\hbar\omega}{2}}$$">The uncertainty of the momentum operator, $\Delta\hat{p}_\alpha$ </div>

                <div class="draggable-element" data-answer="independent" data-correct-text="Not quite. Remember that $$\langle \hat{H}\rangle_\alpha(t)=\hbar\omega\left(\left|\alpha_0\right|^2+\frac{1}{2}\right)$$" data-wrong-text="True! There's a $t$ in $$\langle \hat{H}\rangle_\alpha(t)=\hbar\omega\left(\left|\alpha_0\right|^2+\frac{1}{2}\right)$$">The expectation value of energy, $\langle \hat{H}\rangle_\alpha(t)$ </div>
                <div class="draggable-element" data-answer="independent" data-correct-text="True! There's no $t$ in $$\Delta\hat{H}_\alpha=\hbar\omega\class{subtip alphanorm}{\left|\alpha\right|}$$" data-wrong-text="Not quite. Remember that $$\Delta\hat{H}_\alpha=\hbar\omega\class{subtip alphanorm}{\left|\alpha\right|}$$">The uncertainty of the energy operator, $\Delta\hat{H}_\alpha$ </div>
            </div>



            <div class="drag_title">
                <h3 class="left_title">Time independent </h3>
                <h3 class="right_title">Time dependent</h3>
            </div>
            <div class="left" data-answer="independent">
            </div>
            <div class="right" data-answer="dependent">

            </div>

        </div>

    </article>
</body>