<!DOCTYPE html>

<head>
    <title>Dirac Notation: The Essentials</title>
    <link rel="icon" type="image/x-icon" href="./favicon-32x32.png">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
    <script src="https://unpkg.com/@popperjs/core@2"></script>
    <script src="https://unpkg.com/tippy.js@6"></script>
    <link rel="stylesheet" href="https://unpkg.com/tippy.js@6/themes/material.css" />

    <!-- Run MathJax -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [
                    ['$', '$'],
                    ['\\(', '\\)']
                ],
                tags: 'ams'
            },
            startup: {
                pageReady() {
                    return MathJax.startup.defaultPageReady().then(function() {
                        $.getScript("./resources/diracNotation.js");
                        $.getScript("./resources/main.js");
                    });
                }
            }
        }
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel='stylesheet' href="./resources/theme.css">
    <link rel='stylesheet' href='./resources/style.css'>
    <link rel='stylesheet' href='./resources/dragula_style.css'>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/tiny-slider/2.9.4/tiny-slider.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/tiny-slider/2.9.2/min/tiny-slider.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/dragula/3.6.6/dragula.min.js" integrity="sha512-MrA7WH8h42LMq8GWxQGmWjrtalBjrfIzCQ+i2EZA26cZ7OBiBd/Uct5S3NP9IBqKx5b+MMNH1PhzTsk6J9nPQQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
</head>



<body>
    <br>
    <time pubdate datetime="2021-12-06" title="December 6th, 2021"><span>DEC.06.2021</span></time>

    <h1>
        Dirac Notation:<br>The essentials
    </h1>

    <address class="author"><span>By Frederik Hardervig</span></address>
    <article>
        <section>
            <h2>Complex Numbers</h2>
            <p>I will assume you're already somewhat comfortable with complex numbers, where we introduce the imaginary unit $i=\sqrt{-1}$ to construct complex numbers of the form $z=a+bi$. In quantum mechanics, we often encounter <em>complex conjugates</em>                and the <em>euclidean norm</em>, which we will simply refer to as the <em>norm</em>.</p>
            <p>The complex conjugate of a complex number $z=a+bi$ is denoted with an asterisk, *, and given as $z^\ast=a-bi$. It's effect is to flip the sign of the imaginary part of any complex number, which also means that it is its <span class="itTooltip"
                    data-content="In math terms, we'd say it's an <em>involution</em>.">own inverse</span>: $$\left(z^\ast\right)^\ast=\left((a+bi)^\ast\right)^\ast=\left(a-bi\right)^\ast=a+bi=z$$ It is distributive with matrix-like objects, e.g. the vector
                $\vec{v}=\begin{pmatrix}a+bi\\c+di\end{pmatrix}$ becomes $\vec{v}^\ast=\begin{pmatrix}a-bi\\c-di\end{pmatrix}$. The complex conjugate has no effect on real numbers as there is no imaginary part to change the sign of, e.g. $3^\ast=3$, while
                it's like multiplying by $-1$ for purely imaginary numbers e.g. $3i^\ast=-3i$.</p>
            <p>The norm of a complex number, $z=a+bi$ is given as $$|z|=\sqrt{zz^\ast}=\sqrt{(a+bi)(a-bi)}=\sqrt{a^2+b^2}$$ and carries with it a notion of magnitude, as it returns a real number that's equal to the length of the vector $(a,b)$ in <span class="itTooltip"
                    data-content="I.e. a 2 dimensional coordinate system, where each coordinate is a real number.">$\mathbb{R}^2$</span>. Now, we're ready to introduce Dirac notation.</p>
        </section>
        <section>
            <h2>The Ket</h2>
            <p>
                At the heart of Dirac notation lies the <strong> ket</strong>, $\ket{\phantom{\psi}}$, which is used to denote the <span class="itTooltip" data-content="In general you can work with quantum mechanics from the perspective of wave mechanics or matrix mechanics, which are equivalent. <br>We will focus on matrix mechanics.">vector form</span>                of a <strong>quantum state</strong>. E.g. we could write a quantum state <span class="itTooltip" data-content="$\psi$ and $\varphi$ are commonly used to denote arbitrary quantum states, but we could just as well call them $Q$ and $\epsilon$."> $\psi$</span>                as $\ket{\psi}$. Mathematically, the ket is a vector in a <span class="itTooltip" data-content="If you're curious, <a href='https://www.w3schools.com'>here</a> is another post explaining what a Hilbert space is."><strong>Hilbert space</strong></span>,
                which, for now, you should just know is a <strong>vector of complex numbers</strong>, $$\ket{\psi}=\begin{pmatrix}a\vec{e_1}\\b\vec{e_2}\\\vdots\\n\vec{e_n}\end{pmatrix}$$ where $a,b,...,n\in\mathbb{C}$ and $\{\vec{e_1},\vec{e_2},...,\vec{e_n}\}$
                is our basis.</p>
            <p>
                For now, we will constrain out kets to represent finite vectors, but once we move on to wave mechanics, $\ket{\psi}$ might instead be some vector-valued function dependent on position and time $\ket{\psi(x,y,z,t)}$. Throughout this article, we will use
                the spin state of an electron as our example, but note that <strong>kets can have infinitely many dimensions</strong>.</p>
            <p>
                For a simple system, such as the spin state of an electron, once we measure, it can either be up or down, which we'll denote as $\ket{\uparrow}$ and $\ket{\downarrow}$ respectively, and which we'll use as our basis. Since there are two, we'll need a 2
                dimensional vector to explain this, and we let $$\ket{\uparrow}=\begin{pmatrix}1\\0\end{pmatrix} \;\text{ and }\; \ket{\downarrow}=\begin{pmatrix}0\\1\end{pmatrix}$$ to form an orthonormal basis.
                <strong>Remember the numerical vectors that these kets represent!</strong> They will make the remaining text much more understandable!
            </p>
            <section class="subsection">
                <h3>Superposition</h3>
                <p>
                    Note that I mentioned that the spin of the electron can be either up or down <em>upon measuring</em>. This is a crucial distinction since the quantum state before measurement can be a <span class="itTooltip" data-content="A linear combination is given as $C = a\mathbf{x}+b\mathbf{y}...+n\mathbf{z}$ where $a,b,c$ are scalars, and $x,y,z$ are the objects we're combining.">linear combination</span>                    of up and down.
                </p>
                <p>To explain superposition in the context of qubits, I will present the analogy of flipping a coin. Upon landing, it can only be either heads or tails, but while spinning in the air, one could argue that its state is better described as
                    "50% heads and 50% tails".
                    <span class="itTooltip" data-content="<a href='https://youtu.be/kv-YXKRUheQ' target='_blank'>This</a> computerphile video does an excellent job of clarifying this, and gives a good introduction to the idea of superposition."><strong>The coin is not both heads and tails at the same time</strong></span>                    as some pop-sci articles might lead you to believe. If you think about the probability of each outcome, then instead of the coin "being both at the same time", you simply add them together to get the probability distribution of the
                    state. This is what is meant by superposition, and to complete the analogy you can think of heads as $\ket{\uparrow}$ and tails as $\ket{\downarrow}$.</p>
                <img src="./Linear Combination2.png" style="object-fit:contain; max-width:100%;
max-height:100%;">
                <figcaption>By creating a linear combination of the outcome probabilities, we can describe the superposition between heads and tails.</figcaption>

                <p> In quantum mechanics, we will not exactly be weighing the outcomes by their probabilities directly, but rather by their <strong>probability amplitudes</strong>, e.g.$$\ket{\psi}=\alpha\ket{\uparrow}+\beta\ket{\downarrow}=\begin{pmatrix}\alpha\\\beta\end{pmatrix}$$
                    where $\alpha$ and $\beta$ are the probability amplitudes which are <strong>complex numbers</strong>. The actual probability is obtained by taking the <span class="itTooltip" data-content="Remember that the norm of a complex number, $z=a+bi$, is given as $$\begin{align*}|z|=\sqrt{zz^\ast}&=\sqrt{(a+bi)(a-bi)}\\&=\sqrt{a^2+b^2}\end{align*}$$">norm squared</span>                    of our amplitude. If we let our probability amplitude of spin up be the complex number $\alpha=a+bi$, then the probability of spin up, $P_\uparrow$, is given as $$\begin{equation}P_\uparrow=|\alpha|^2=\sqrt{a^2+b^2}^2=a^2+b^2 \label{eq:Pup}\end{equation}$$
                    while the probability of spin down, $P_\downarrow$, is given as $$P_\downarrow=|\beta|^2=\sqrt{c^2+d^2}^2=c^2+d^2$$ where we let $\beta=c+di$.</p>

            </section>
            <p>
                <h3>Recap:</h3>
                We can describe quantum states as <span id="ilqKet1">____</span> vectors, with entries corresponding to the <span id="ilqKet2">____</span> of measuring an outcome as the corresponding basis vector.
            </p>
            <p>We will now do a small detour from Dirac notation, to introduce some matrix operations.</p>
        </section>
        <section>
            <h2>Transpose</h2>
            <p>To <em>transpose</em> is to switch the rows and columns in matrix-like objects while making no changes to the entries themselves. E.g. $$\begin{bmatrix}a&b\\c&d\\e&f\end{bmatrix}^T=\begin{bmatrix}a&c&e\\b&d&f\end{bmatrix} \;\text{ and }\;
                \begin{pmatrix}a\\b\\c\end{pmatrix}^T=\begin{pmatrix}a&b&c\end{pmatrix}$$ Like the complex conjugate, transposing is an involution, i.e. transposing the same matrix twice leaves us back with the initial matrix: $$\left(A^T\right)^T=\left(\begin{bmatrix}a&b\\c&d\end{bmatrix}^T\right)^T=\begin{bmatrix}a&c\\b&d\end{bmatrix}^T=\begin{bmatrix}a&b\\c&d\end{bmatrix}=A$$

            </p>
        </section>
        <section>
            <h2>Adjoint</h2>
            <p>The <span class="itTooltip" data-content="Sometimes called the conjugate transpose, Hermitian transpose or Hermitian conjugate"><em>adjoint</em></span> of a matrix-like object is the <strong>same as taking the complex conjugate of the
                transposed</strong> matrix. In quantum mechanics it is denoted with a dagger, $A^\dagger$. Letting $$\begin{align*} A&=\begin{bmatrix}a+bi&c+di\\e+fi&g+hi\end{bmatrix}\\ &\text{then}\\ A^\dagger&={\left(A^\ast\right)^T}\\&=\begin{bmatrix}a-bi&c-di\\e-fi&g-hi\end{bmatrix}^T\\&=\begin{bmatrix}a-bi&e-fi\\c-di&g-hi\end{bmatrix}
                \end{align*}$$ Just like both transposing and complex conjugates are involutions, it also holds that $\left(A^\dagger\right)^\dagger=A$. Feel free to confirm this by yourself using the example above.</p>
        </section>
        <section>
            <h2>Bra</h2>
            <p>
                Returning to our electron spin quantum state, we can take the adjoint of $\ket{\psi}$ in order to obtain: $$\ket{\psi}^\dagger=\begin{pmatrix}\alpha\\\beta\end{pmatrix}^\dagger=\begin{pmatrix}\alpha^\ast&\beta^\ast\end{pmatrix}=\bra{\psi}$$ What's this,
                our ket flipped? That's right, we're introducing the one and only <strong>bra</strong>, $\bra{\phantom{\psi}}$. The adjoint even let's us convert back and forth between kets and bras. $$\bra{\psi}^\dagger=\begin{pmatrix}\alpha^\ast&\beta^\ast\end{pmatrix}^\dagger=\begin{pmatrix}\alpha\\\beta\end{pmatrix}=\ket{\psi}$$
            </p>
        </section>

        <section>
            <h2>
                Inner Product
            </h2>
            <p>Finally we know enough to begin seeing why Dirac notation might be useful. The vertical bars almost seems to fit together, and writing $\braket{\psi|\psi}$ we've made our first <strong>bra-ket pair</strong>! We say that this will give us the
                inner product of $\ket{\psi}$ with respect to itself. The <em>inner product</em> is an extension of the dot product, but will in this reading behave the exact same. As such, it takes in two vectors and <strong>returns a scalar</strong>.
                Writing out the vectors and performing matrix multiplication, we obtain: $$\begin{equation}\braket{\psi|\psi}=\begin{pmatrix}\alpha^\ast&\beta^\ast\end{pmatrix}\begin{pmatrix}\alpha\\\beta\end{pmatrix}=\alpha^\ast\alpha+\beta^\ast\beta=\class{tooltip}{\cssId{sumNormSqrd}{|\alpha|}}^2+|\beta|^2\qquad
                \label{eq:selfprj}\end{equation}$$ This looks familiar, namely in eq. $\ref{eq:Pup}$, we saw that $P_\uparrow=|\alpha|^2$, and by extension that $P_\downarrow=|\beta|^2$. As such, by taking the inner product of a state with respect to
                itself we've extracted the sum of probabilities of all outcomes. </p>
            <p>For this to make sense physically we need to have a properly normalized state such that, $\braket{\psi|\psi}=1$, much like we need the integral of probability density functions to <span class="itTooltip" data-content="In other words, upon measuring the particle must always be in <em>some</em> state. It's not allowed to simply cease to exist.">equal 1</span>.
                Note that the inner product of a quantum state with itself, $\braket{\psi|\psi}$, is a special case of the inner product where the resulting probability amplitude and associated probability is the same.
            </p>
            <p>While this fact is useful for normalizing our states, the inner product also serves as an exceedingly useful tool since we can use it to extract probability amplitudes of <strong>specific outcomes</strong>. E.g. $$\begin{equation}\braket{\uparrow|\psi}=\begin{pmatrix}1&0\end{pmatrix}\begin{pmatrix}\alpha\\\beta\end{pmatrix}=1\alpha+0\beta=\alpha\label{eq:ampup}\end{equation}$$
                where we recall that $\alpha$ is the probability amplitude of $\ket{\psi}$ for the outcome $\ket{\uparrow}$. From eq. \ref{eq:selfprj} and eq. \ref{eq:ampup}, we see that $$\braket{\psi|\psi}=|\alpha|^2+|\beta|^2=\left|\braket{\uparrow|\psi}\right|^2+\left|\braket{\downarrow|\psi}\right|^2$$
                which hints at the fact that we might be able to define an alternative construction of our quantum state by using $\braket{e_n|\psi}$ as the amplitude for basis vector $\vec{e_n}$. For our two state system, instead of $$\ket{\psi}=\alpha\ket{\uparrow}+\beta\ket{\downarrow}=\begin{pmatrix}\alpha\\\beta\end{pmatrix}$$
                we could say $$\ket{\psi}=\class{tooltip}{\cssId{numvec}{\braket{\uparrow|\psi}\ket{\uparrow}}}+\braket{\downarrow|\psi}\ket{\downarrow}=\begin{pmatrix}\alpha\\\beta\end{pmatrix}$$ For higher dimensional states with orthonormal basis $\{\vec{e_1},\vec{e_2},...,\vec{e_n}\}$,
                with probability amplitudes $a_1, a_2, ..., a_n$, $$\begin{equation}\ket{\psi}=\sum_{i=1}^n a_i\ket{e_i}=\sum_{i=1}^n\braket{e_i|\psi}\ket{e_i}\label{eq:sumstate}\end{equation}$$
            </p>
            <div class="example tight">
                <button class="collapsible">Aside: Inner products as projections</button>
                <div class="content">
                    <p>
                        I will now cover a bit of geometric intuition for the inner product. Recall that inner products are a generalization of dot products, and that dot products can be used to project one vector, $\vec{v}$, onto another, $\vec{b}$, by using $\vec{v}_{proj\rightarrow
                        b}=(\hat{b}\cdot\vec{v})\hat{b}$ where $\hat{b}$ is the unit vector of $\vec{b}$. As mentioned above, for our quantum states to make physical sense, any ket we make must be normalized, and thus the unit vector of any state is the
                        state itself $\hat{\ket{\psi}}=\ket{\psi}$. Re-writing the projection above in Dirac notation, we thus obtain $\ket{\psi_{proj\rightarrow \phi}}=$<span id="ilqInPP1">____</span>, which looks exactly like
                        the summation in eq. \ref{eq:sumstate}.
                    </p>
                </div>

            </div>
            <p>The inner product carries with it a notion of orthogonality. E.g. we mentioned that our basis $\{\ket{\uparrow},\ket{\downarrow}\}$ is orthonormal. This can be easily confirmed using inner products. $$\begin{align*} \braket{\uparrow|\uparrow}&=\begin{pmatrix}1&0\end{pmatrix}\begin{pmatrix}1\\0\end{pmatrix}=1\cdot1+0\cdot0=1\\
                \braket{\uparrow|\downarrow}&=\begin{pmatrix}1&0\end{pmatrix}\begin{pmatrix}0\\1\end{pmatrix}=0\cdot1+1\cdot0=0\\ \braket{\downarrow|\uparrow}&=\begin{pmatrix}0&1\end{pmatrix}\begin{pmatrix}1\\0\end{pmatrix}=1\cdot0+0\cdot1=0\\ \braket{\downarrow|\downarrow}&=\begin{pmatrix}0&1\end{pmatrix}\begin{pmatrix}0\\1\end{pmatrix}=0\cdot0+1\cdot1=1
                \end{align*}$$ Using our intuition from dot products, we see that $\ket{\uparrow}$ and $\ket{\downarrow}$ are orthogonal since $\braket{\uparrow|\downarrow}=\braket{\downarrow|\uparrow}=0$, and normalized since $\braket{\uparrow|\uparrow}=\braket{\downarrow|\downarrow}=1$.
                <p class="focus">
                    It is very common to work with orthonormal bases like these, which means that $\braket{e_i|e_j}=\delta_{ij}$, where $\delta_{ij}$ is the Kronecker delta defined as $$\delta_{ij}=\left\lbrace\begin{matrix} 0\quad \text{if } i\neq j\\1\quad \text{if } i=
                    j \end{matrix}\right.$$
                </p>
            </p>
            <p>Re-doing our calculation for $\braket{\psi|\psi}$ where instead of using $\ket{\psi}=\begin{pmatrix}\alpha\\\beta\end{pmatrix}$ we will use $\ket{\psi}=\alpha\ket{\uparrow}+\beta\ket{\downarrow}$, and the fact that we have a orthonormal basis:
                <div class="sbs-proof">
                    <div class="proof-navigation">
                        <button class="proof-prev-button" , onclick="prevProofStep(this.value)">&#9651</button>
                        <button class="proof-next-button" , onclick="nextProofStep(this.value)">&#9661</button>
                        <button class="proof-reset-button" , onclick="resetProof(this.value)">&#8634</button>
                        <button class="proof-expand-button" , onclick="expandProof(this.value)">&#x26F6</button>
                    </div>
                    <div class="proof-equations">
                        <div>$\begin{align*} \braket{\psi|\psi}&=\left(\alpha^\ast\bra{\uparrow}+\beta^\ast\bra{\downarrow}\right)\left(\alpha\ket{\uparrow}+\beta\ket{\downarrow}\right) \\ &=\alpha^\ast\alpha\underbrace{\braket{\uparrow|\uparrow}}_{\delta_{\uparrow\uparrow}=\,1}+\alpha^\ast\beta\underbrace{\braket{\uparrow|\downarrow}}_{\delta_{\uparrow\downarrow}=\,0}+\beta^\ast\alpha\underbrace{\braket{\downarrow|\uparrow}}_{\delta_{\downarrow\uparrow}=\,0}+\beta^\ast\beta\underbrace{\braket{\downarrow|\downarrow}}_{\delta_{\downarrow\downarrow}=\,1}\\
                            &=|\alpha|^2+|\beta|^2 \end{align*}$
                        </div>
                    </div>
                    <div class="proof-explanation">
                        <div class="step">Step 1: $$\bra{\psi}=\class{tooltip}{\cssId{conjBra}{\alpha^\ast\bra{\uparrow}}}+\beta^\ast\bra{\downarrow}\;\text{ multiplied by }\;\ket{\psi}=\alpha\ket{\uparrow}+\beta\ket{\downarrow}$$</div>
                        <div class="step">Step 2: When distributing the parentheses, we can move around scalars freely, but must maintain the order of bras and kets.</div>
                        <div class="step">Step 3: Since $\ket{\uparrow}$ and $\ket{\downarrow}$ are <span class="itTooltip" data-content="$$\begin{align*} \braket{\uparrow|\uparrow}&=1\\
                \braket{\uparrow|\downarrow}&=0\\ \braket{\downarrow|\uparrow}&=0\\ \braket{\downarrow|\downarrow}&=1
                \end{align*}$$">orthonormal</span>, we can use the shown Kronecker deltas to simplify the equation. Additionally, we recall that norms are given as $|\alpha|=\sqrt{\alpha^\ast \alpha}$.</div>
                    </div>

                </div>
                Once we move onto an $x,y,z$ basis, the usefulness of this fact will become even clearer, but already now, the intuitiveness of Dirac notation should begin to shine through.</p>

            <p>
                <h3>Recap:</h3>
                Inner products extract probability amplitudes. The bra-ket pair takes two <span id="ilqInP1">____</span> and outputs a <span id="ilqInP2">____</span>. The ket is our quantum state which we project onto the bra, in order to obtain the probability
                amplitude of the ket being in the state of the bra. This is an incredibly powerful tool and as we've seen can also be used to define our quantum states. Lastly, by working with orthonormal bases we can quickly remove terms and simplify
                our equations using the <span id="ilqInP3">____</span> delta.
            </p>
        </section>
        <section>
            <h2>
                Operators
            </h2>
            <p>Operators are matrices, and <strong>act as transformations</strong> on our quantum states, just like the transformation matrices from linear algebra. To apply an operator to a ket, you left multiply, $X\ket{\psi}$, while you right multiply
                to apply them to bras, $\bra{\psi}Y$. Being matrices, they are commutative and associative $$\begin{align*} X+Y&=Y+X\\ X+(Y+Z)&=(X+Y)+Z \end{align*}$$ and all the operators we will be encountering are linear, they satisfy $$X(c_\psi\ket{\psi}+c_\phi\ket{\phi})=c_\psi
                X\ket{\psi}+c_\phi X\ket{\phi}$$</p>
            <p>Importantly, <strong>in general, operators do not <span class="itTooltip" data-content="We'll get back to where this word comes from at the end of this article"><em>commute</em></span></strong>, i.e. $XY\neq YX$. Operators are said to be
                <em>Hermitian</em> if they equal their <span class="itTooltip" data-content="Recall the adjoint is denoted by the dagger, $\dagger$, and represents taking the transpose and complex conjugate, i.e. $X^\dagger={X^T}^\ast$">adjoint</span>,
                $X=X^\dagger$. Operators are said to be <em>unitary</em> if $U^\dagger U=UU^\dagger =\mathbb{I}$, where $\mathbb{I}$ is the identity matrix. Notice that the order of operators is <span class="itTooltip" data-content="Sometimes know as being anti-distributive">swapped</span>                when the adjoint is distributed: $$(XY)^\dagger=Y^\dagger X^\dagger$$
            </p>
            <p>A concrete example of an operator is the Pauli-x operator in 2d, $\sigma_x=\begin{pmatrix}0&1\\1&0\end{pmatrix}$, which flips the amplitudes of the quantum state. As a short (optional) exercise, what quantum state does $\sigma_x\ket{\uparrow}$
                equal?
                <br> Answer:
                <span id="ilqOP1">____</span>
            </p>
            <p class="focus">
                We will be working a lot with operators acting on an <em>eigenstate</em> (i.e. an eigenvector). Re-call from linear algebra that this means that applying the operator to the state will return a scalar multiple of the state, $X_\psi\ket{\psi}=c_\psi\ket{\psi}$,
                where $c_\psi$ is the eigenvalue associated with the eigenstate and operators. For hermitian operators the eigenvalue will always be real, while non-hermitian operators might have complex eigenvalues.
            </p>
        </section>

        <section>
            <h2>Outer Products</h2>
            <p>
                Now that we've seen that the inner product $\braket{\phi|\psi}$ provides a scalar, we can begin thinking about the outer product $\ket{\psi}\bra{\phi}$. This results in an operator, i.e. a matrix. The adjoint of an operator $X=\ket{\psi}\bra{\phi}$, is
                given as $X^\dagger=\ket{\phi}\bra{\psi}$. As we've shown in eq. $\ref{eq:sumstate}$, we can denote a quantum state by a summation, $$\begin{equation}\ket{\psi}=\sum_{i=1}^n a_i\ket{e_i}=\sum_{i=1}^n \ket{e_i}a_i=\sum_{i=1}^n\ket{e_i}\braket{e_i|\psi}\label{eq:outerstate}\end{equation}$$
                Since kets and bras are <span class="itTooltip" data-content="$(\ket{\psi}\bra{\phi})\cdot\ket{\gamma}=\ket{\psi}\cdot(\braket{\phi|\gamma})$"><em>associative</em></span>, eq. $\ref{eq:outerstate}$ shows that <span class="itTooltip"
                    data-content="$$\begin{align*}
\mathbb{I}\ket{\psi}&=\ket{\psi}\\
&=\sum_{i=1}^n\ket{e_i}\braket{e_i|\psi}\\
&=\left(\sum_{i=1}^n\ket{e_i}\bra{e_i}\right)\ket{\psi}\\
\end{align*}$$">$\sum_{i=1}^n\ket{e_i}\bra{e_i}=\mathbb{I}$</span>.</p>
            <p>The result above is known as closure or the completeness relation. The reason this is worth pointing out, is that it gives us an invaluable tool for our algebra, namely that much like we can multiply by 1 wherever we want in regular algebra,
                we can likewise insert $\sum_{i=1}^n\ket{e_i}\bra{e_i}$ wherever we want.</p>
            <div class="example tight">
                <button class="collapsible">Aside: "Illegal Products"</button>
                <div class="content">
                    <p>
                        To emphasize the rules of multiplication in Dirac notation, the following products are non-sensical: $$\ket{\psi}X\qquad X\bra{\psi}\qquad \ket{\psi}\ket{\phi}\qquad \bra{\psi}\bra{\phi}$$ When you get further into quantum mechanics, you might begin encountering
                        the latter two (ket times ket, and bra times bra), but that's because they're shorthand for the tensor product, $\ket{\psi}\otimes\ket{\phi}$, between two kets from different vector spaces.
                    </p>
                </div>
            </div>
        </section>

        <section>
            <h2>Commutators</h2>
            <p>
                Remember how we said that $X$ and $Y$ generally don't commutate, which is the same as stating that $XY\neq YX$? From this, we can build an object, the <strong>commutator</strong>, which is $0$ if two operators commutate. We denote it as
                $[X,Y]=XY-YX$, which is clearly only $0$ if $XY=YX$.
            </p>
        </section>
        <section>
            <h2>Final thoughts</h2>
            Hello</section>

    </article>

    <div style="min-height: 100px;"></div>
    <p><strong>The following was made as a study-guide quiz for a lesson on quantum harmonic oscillators, focusing on quasi-classical/coherent states.</strong></p>
    <div style="min-height: 10px;"></div>
    <time pubdate datetime="2021-11-28" title="November 28th, 2021"><span>NOV.28.2021</span></time>

    <h1>
        Coherent State Quiz
    </h1>

    <address class="author"><span>By Frederik Hardervig</span></address>
    <p>Sort the quantities below into time independent and time dependent groups by dragging them. Hover after moving to receive feedback. It is assumed that the quantities are for coherent states. E.g. Instead of saying "The uncertainty of the position
        operator for coherent states" we simply say "The uncertainty of the position operator" and put an $\alpha$ in the subscript.</p>
    <div class="draggable">
        <h3>Drag these quantities to the cells below</h3>
        <div class="top" data-answer="default">
            <div class="draggable-element" data-answer="dependent" data-correct-text="True! There's a $t$ in $$\left\langle \hat{x}\right\rangle_\alpha(t)=\sqrt{\frac{2\hbar}{m\omega}}\left|\alpha_0\right|\cos{\left(\omega t-\phi\right)}$$" data-wrong-text="Not quite. Remember that $$\left\langle \hat{x}\right\rangle_\alpha(t)=\sqrt{\frac{2\hbar}{m\omega}}\left|\alpha_0\right|\cos{\left(\omega t-\phi\right)}$$">The expectation value of position, $\left\langle \hat{x}\right\rangle_\alpha(t)$ </div>
            <div class="draggable-element" data-answer="independent" data-correct-text="True! There's no $t$ in $$\Delta\hat{x}_\alpha=\sqrt{\frac{\hbar}{2m\omega}}$$" data-wrong-text="Not quite. Remember that $$\Delta\hat{x}_\alpha=\sqrt{\frac{\hbar}{2m\omega}}$$">The uncertainty of the position operator, $\Delta\hat{x}_\alpha$ </div>



            <div class="draggable-element" data-answer="dependent" data-correct-text="True! There's a $t$ in $$\left\langle \hat{p}\right\rangle_\alpha(t)=-\sqrt{2m\hbar\omega}\left|\alpha_0\right|\sin{\left(\omega t-\phi\right)}$$" data-wrong-text="Not quite. Remember that $$\left\langle \hat{p}\right\rangle_\alpha(t)=-\sqrt{2m\hbar\omega}\left|\alpha_0\right|\sin{\left(\omega t-\phi\right)}$$">The expectation value of momentum, $\left\langle \hat{p}\right\rangle_\alpha(t)$ </div>
            <div class="draggable-element" data-answer="independent" data-correct-text="True! There's no $t$ in $$\Delta\hat{p}_\alpha=\sqrt{\frac{m\hbar\omega}{2}}$$" data-wrong-text="Not quite. Remember that $$\Delta\hat{p}_\alpha=\sqrt{\frac{m\hbar\omega}{2}}$$">The uncertainty of the momentum operator, $\Delta\hat{p}_\alpha$ </div>

            <div class="draggable-element" data-answer="independent" data-correct-text="True! There's no $t$ in $$\langle \hat{H}\rangle_\alpha(t)=\hbar\omega\left(\left|\alpha_0\right|^2+\frac{1}{2}\right)$$" data-wrong-text="Not quite. Remember that $$\langle \hat{H}\rangle_\alpha(t)=\hbar\omega\left(\left|\alpha_0\right|^2+\frac{1}{2}\right)$$">The expectation value of energy, $\langle \hat{H}\rangle_\alpha(t)$ </div>
            <div class="draggable-element" data-answer="independent" data-correct-text="True! There's no $t$ in $$\Delta\hat{H}_\alpha=\hbar\omega\class{subtip alphanorm}{\left|\alpha\right|}$$" data-wrong-text="Not quite. Remember that $$\Delta\hat{H}_\alpha=\hbar\omega\class{subtip alphanorm}{\left|\alpha\right|}$$">The uncertainty of the energy operator, $\Delta\hat{H}_\alpha$ </div>
        </div>



        <div class="drag_title">
            <h3 class="left_title">Time independent </h3>
            <h3 class="right_title">Time dependent</h3>
        </div>
        <div class="left" data-answer="independent">
        </div>
        <div class="right" data-answer="dependent">

        </div>

    </div>
    <div style="min-height: 300px;"></div>
</body>